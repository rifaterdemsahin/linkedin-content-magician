import os
import argparse
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from markdown_it import MarkdownIt

def get_text_from_markdown(file_path):
    """Reads a markdown file and returns its text content."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            md = MarkdownIt()
            content = f.read()
            tokens = md.parse(content)
            text_content = " ".join([token.content for token in tokens if token.type == 'inline'])
            return text_content
    except Exception as e:
        print(f"âŒ Error reading or parsing {file_path}: {e}")
        return None

def main():
    parser = argparse.ArgumentParser(description="Index markdown files for semantic search with debug output.")
    parser.add_argument('--folder', type=str, required=True, help="Folder containing markdown files to index.")
    args = parser.parse_args()

    folder_path = args.folder
    if not os.path.isdir(folder_path):
        print(f"âŒ Error: Folder not found at {folder_path}")
        return

    print("ğŸ”„ Loading sentence transformer model...")
    model = SentenceTransformer('all-MiniLM-L6-v2')
    print("âœ… Model loaded successfully!")

    filepaths = []
    texts = []

    print(f"\nğŸ” Scanning for markdown files in '{folder_path}'...")
    print("=" * 60)
    
    file_count = 0
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.endswith('.md'):
                file_count += 1
                file_path = os.path.join(root, file)
                print(f"\nğŸ“„ Processing file {file_count}: {file}")
                print(f"   ğŸ“ Full path: {file_path}")
                
                text = get_text_from_markdown(file_path)
                if text:
                    # Show preview of extracted text
                    preview = text[:200] + "..." if len(text) > 200 else text
                    print(f"   ğŸ“ Extracted text ({len(text)} chars): {preview}")
                    print(f"   âœ… Successfully indexed!")
                    
                    filepaths.append(file_path)
                    texts.append(text)
                else:
                    print(f"   âŒ Failed to extract text")

    if not texts:
        print("\nâŒ No markdown files found or processed.")
        return

    print(f"\nğŸ”„ Creating embeddings for {len(texts)} files...")
    print("=" * 60)
    
    embeddings = model.encode(texts, convert_to_tensor=False)
    embeddings = np.array(embeddings).astype('float32')
    
    print(f"âœ… Embeddings created! Shape: {embeddings.shape}")
    
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index = faiss.IndexIDMap(index)
    
    ids = np.array(range(len(filepaths)))
    index.add_with_ids(embeddings, ids)

    print(f"\nğŸ’¾ Saving FAISS index to 'faiss_index.bin'...")
    faiss.write_index(index, 'faiss_index.bin')
    print("âœ… Index saved!")
    
    print(f"\nğŸ’¾ Saving file paths to 'filepaths.txt'...")
    with open('filepaths.txt', 'w', encoding='utf-8') as f:
        for i, path in enumerate(filepaths):
            f.write(f"{path}\n")
            print(f"   {i+1}. {os.path.basename(path)}")
    
    print("\nğŸ‰ Indexing complete!")
    print("=" * 60)
    print(f"ğŸ“Š Summary:")
    print(f"   â€¢ Files indexed: {len(texts)}")
    print(f"   â€¢ Embedding dimension: {dimension}")
    print(f"   â€¢ Index type: FAISS IndexFlatL2 with IDMap")
    print(f"   â€¢ Output files: faiss_index.bin, filepaths.txt")

if __name__ == "__main__":
    main()